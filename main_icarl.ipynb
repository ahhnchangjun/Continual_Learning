{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1mILksgbwlcWn5OoFtUcDJx1QMDntku53","authorship_tag":"ABX9TyM+793PUIGMTJbVZ7u68Yvm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RErEHUc6qEXc","executionInfo":{"status":"error","timestamp":1688973486613,"user_tz":-540,"elapsed":512716,"user":{"displayName":"안창준전자ㆍ제어공학과","userId":"03412762459924900416"}},"outputId":"dba1d619-261f-4a6e-bb3a-6ade285787e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Classes in this batch: tensor([87,  0, 52, 58, 44, 91, 68, 97, 51, 15])\n","\n","\n","Before first epoch\n","  validation loss:\t\t29.787861\n","  top 1 accuracy:\t\t0.10 %\n","  top 5 accuracy:\t\t0.90 %\n","Batch of classes number 1 arrives ...\n","0.2545773684978485\n","Batch of classes 1 out of 10 batches\n","Epoch 1 of 70 took 6.627s\n","  training loss:\t\t0.065153\n","  validation loss:\t\t0.026547\n","  top 1 accuracy:\t\t41.50 %\n","  top 5 accuracy:\t\t85.60 %\n","0.024929381906986237\n","Batch of classes 1 out of 10 batches\n","Epoch 2 of 70 took 5.207s\n","  training loss:\t\t0.022517\n","  validation loss:\t\t0.022188\n","  top 1 accuracy:\t\t52.20 %\n","  top 5 accuracy:\t\t93.50 %\n","0.025051536038517952\n","Batch of classes 1 out of 10 batches\n","Epoch 3 of 70 took 6.359s\n","  training loss:\t\t0.020457\n","  validation loss:\t\t0.020533\n","  top 1 accuracy:\t\t55.80 %\n","  top 5 accuracy:\t\t94.70 %\n","0.018849622458219528\n","Batch of classes 1 out of 10 batches\n","Epoch 4 of 70 took 5.178s\n","  training loss:\t\t0.018642\n","  validation loss:\t\t0.017874\n","  top 1 accuracy:\t\t61.20 %\n","  top 5 accuracy:\t\t95.60 %\n","0.015818044543266296\n","Batch of classes 1 out of 10 batches\n","Epoch 5 of 70 took 5.803s\n","  training loss:\t\t0.017077\n","  validation loss:\t\t0.017840\n","  top 1 accuracy:\t\t62.80 %\n","  top 5 accuracy:\t\t96.00 %\n","0.01586495153605938\n","Batch of classes 1 out of 10 batches\n","Epoch 6 of 70 took 6.197s\n","  training loss:\t\t0.016167\n","  validation loss:\t\t0.015948\n","  top 1 accuracy:\t\t66.80 %\n","  top 5 accuracy:\t\t96.30 %\n","0.015557652339339256\n","Batch of classes 1 out of 10 batches\n","Epoch 7 of 70 took 5.158s\n","  training loss:\t\t0.015087\n","  validation loss:\t\t0.016993\n","  top 1 accuracy:\t\t64.70 %\n","  top 5 accuracy:\t\t97.00 %\n","0.014791096560657024\n","Batch of classes 1 out of 10 batches\n","Epoch 8 of 70 took 6.612s\n","  training loss:\t\t0.014824\n","  validation loss:\t\t0.015075\n","  top 1 accuracy:\t\t69.10 %\n","  top 5 accuracy:\t\t97.30 %\n","0.013648400083184242\n","Batch of classes 1 out of 10 batches\n","Epoch 9 of 70 took 5.163s\n","  training loss:\t\t0.014539\n","  validation loss:\t\t0.014325\n","  top 1 accuracy:\t\t71.30 %\n","  top 5 accuracy:\t\t97.00 %\n","0.01377124059945345\n","Batch of classes 1 out of 10 batches\n","Epoch 10 of 70 took 5.873s\n","  training loss:\t\t0.013197\n","  validation loss:\t\t0.013784\n","  top 1 accuracy:\t\t71.80 %\n","  top 5 accuracy:\t\t98.60 %\n","0.014256679452955723\n","Batch of classes 1 out of 10 batches\n","Epoch 11 of 70 took 6.022s\n","  training loss:\t\t0.012767\n","  validation loss:\t\t0.016307\n","  top 1 accuracy:\t\t66.50 %\n","  top 5 accuracy:\t\t96.70 %\n","0.014637889340519905\n","Batch of classes 1 out of 10 batches\n","Epoch 12 of 70 took 5.368s\n","  training loss:\t\t0.011884\n","  validation loss:\t\t0.013216\n","  top 1 accuracy:\t\t73.70 %\n","  top 5 accuracy:\t\t98.10 %\n","0.011492862366139889\n","Batch of classes 1 out of 10 batches\n","Epoch 13 of 70 took 6.695s\n","  training loss:\t\t0.011698\n","  validation loss:\t\t0.015584\n","  top 1 accuracy:\t\t72.10 %\n","  top 5 accuracy:\t\t97.50 %\n","0.014229738153517246\n","Batch of classes 1 out of 10 batches\n","Epoch 14 of 70 took 5.185s\n","  training loss:\t\t0.011662\n","  validation loss:\t\t0.012344\n","  top 1 accuracy:\t\t75.80 %\n","  top 5 accuracy:\t\t98.90 %\n","0.010981819592416286\n","Batch of classes 1 out of 10 batches\n","Epoch 15 of 70 took 6.138s\n","  training loss:\t\t0.010535\n","  validation loss:\t\t0.012065\n","  top 1 accuracy:\t\t76.60 %\n","  top 5 accuracy:\t\t98.40 %\n","0.010686342604458332\n","Batch of classes 1 out of 10 batches\n","Epoch 16 of 70 took 5.806s\n","  training loss:\t\t0.011094\n","  validation loss:\t\t0.017730\n","  top 1 accuracy:\t\t68.70 %\n","  top 5 accuracy:\t\t97.00 %\n","0.01353379711508751\n","Batch of classes 1 out of 10 batches\n","Epoch 17 of 70 took 5.248s\n","  training loss:\t\t0.012049\n","  validation loss:\t\t0.013295\n","  top 1 accuracy:\t\t74.10 %\n","  top 5 accuracy:\t\t98.30 %\n","0.009536300785839558\n","Batch of classes 1 out of 10 batches\n","Epoch 18 of 70 took 6.774s\n","  training loss:\t\t0.010243\n","  validation loss:\t\t0.011528\n","  top 1 accuracy:\t\t77.50 %\n","  top 5 accuracy:\t\t99.00 %\n","0.007967404089868069\n","Batch of classes 1 out of 10 batches\n","Epoch 19 of 70 took 5.186s\n","  training loss:\t\t0.009082\n","  validation loss:\t\t0.010252\n","  top 1 accuracy:\t\t82.00 %\n","  top 5 accuracy:\t\t98.60 %\n","0.008993010967969894\n","Batch of classes 1 out of 10 batches\n","Epoch 20 of 70 took 6.218s\n","  training loss:\t\t0.009594\n","  validation loss:\t\t0.013654\n","  top 1 accuracy:\t\t73.00 %\n","  top 5 accuracy:\t\t96.80 %\n","0.01095533836632967\n","Batch of classes 1 out of 10 batches\n","Epoch 21 of 70 took 5.526s\n","  training loss:\t\t0.008763\n","  validation loss:\t\t0.010913\n","  top 1 accuracy:\t\t79.00 %\n","  top 5 accuracy:\t\t98.70 %\n","0.008565504103899002\n","Batch of classes 1 out of 10 batches\n","Epoch 22 of 70 took 5.302s\n","  training loss:\t\t0.008338\n","  validation loss:\t\t0.011968\n","  top 1 accuracy:\t\t76.50 %\n","  top 5 accuracy:\t\t98.20 %\n","0.009071324951946735\n","Batch of classes 1 out of 10 batches\n","Epoch 23 of 70 took 6.814s\n","  training loss:\t\t0.008138\n","  validation loss:\t\t0.009978\n","  top 1 accuracy:\t\t81.00 %\n","  top 5 accuracy:\t\t99.10 %\n","0.008546778932213783\n","Batch of classes 1 out of 10 batches\n","Epoch 24 of 70 took 5.232s\n","  training loss:\t\t0.008977\n","  validation loss:\t\t0.010480\n","  top 1 accuracy:\t\t79.60 %\n","  top 5 accuracy:\t\t98.80 %\n","0.008780248463153839\n","Batch of classes 1 out of 10 batches\n","Epoch 25 of 70 took 6.402s\n","  training loss:\t\t0.008003\n","  validation loss:\t\t0.010261\n","  top 1 accuracy:\t\t80.40 %\n","  top 5 accuracy:\t\t98.20 %\n","0.006743178237229586\n","Batch of classes 1 out of 10 batches\n","Epoch 26 of 70 took 5.291s\n","  training loss:\t\t0.007995\n","  validation loss:\t\t0.010175\n","  top 1 accuracy:\t\t80.20 %\n","  top 5 accuracy:\t\t99.00 %\n","0.007215125486254692\n","Batch of classes 1 out of 10 batches\n","Epoch 27 of 70 took 5.721s\n","  training loss:\t\t0.007806\n","  validation loss:\t\t0.014286\n","  top 1 accuracy:\t\t76.30 %\n","  top 5 accuracy:\t\t98.30 %\n","0.009131320752203465\n","Batch of classes 1 out of 10 batches\n","Epoch 28 of 70 took 6.442s\n","  training loss:\t\t0.008240\n","  validation loss:\t\t0.011699\n","  top 1 accuracy:\t\t77.30 %\n","  top 5 accuracy:\t\t99.00 %\n","0.007197234313935041\n","Batch of classes 1 out of 10 batches\n","Epoch 29 of 70 took 5.298s\n","  training loss:\t\t0.007198\n","  validation loss:\t\t0.011247\n","  top 1 accuracy:\t\t79.40 %\n","  top 5 accuracy:\t\t98.00 %\n","0.0082630580291152\n","Batch of classes 1 out of 10 batches\n","Epoch 30 of 70 took 6.686s\n","  training loss:\t\t0.007215\n","  validation loss:\t\t0.009002\n","  top 1 accuracy:\t\t83.30 %\n","  top 5 accuracy:\t\t99.00 %\n","0.007333699148148298\n","Batch of classes 1 out of 10 batches\n","Epoch 31 of 70 took 5.303s\n","  training loss:\t\t0.006153\n","  validation loss:\t\t0.009337\n","  top 1 accuracy:\t\t83.70 %\n","  top 5 accuracy:\t\t99.00 %\n","0.007692461833357811\n","Batch of classes 1 out of 10 batches\n","Epoch 32 of 70 took 6.290s\n","  training loss:\t\t0.006174\n","  validation loss:\t\t0.010700\n","  top 1 accuracy:\t\t80.70 %\n","  top 5 accuracy:\t\t98.80 %\n","0.005898579023778439\n","Batch of classes 1 out of 10 batches\n","Epoch 33 of 70 took 5.801s\n","  training loss:\t\t0.006298\n","  validation loss:\t\t0.009810\n","  top 1 accuracy:\t\t82.30 %\n","  top 5 accuracy:\t\t99.30 %\n","0.005387210287153721\n","Batch of classes 1 out of 10 batches\n","Epoch 34 of 70 took 5.263s\n","  training loss:\t\t0.005492\n","  validation loss:\t\t0.010191\n","  top 1 accuracy:\t\t80.90 %\n","  top 5 accuracy:\t\t98.80 %\n","0.004129630979150534\n","Batch of classes 1 out of 10 batches\n","Epoch 35 of 70 took 6.887s\n","  training loss:\t\t0.005130\n","  validation loss:\t\t0.015446\n","  top 1 accuracy:\t\t74.10 %\n","  top 5 accuracy:\t\t98.00 %\n","0.007421800401061773\n","Batch of classes 1 out of 10 batches\n","Epoch 36 of 70 took 5.232s\n","  training loss:\t\t0.006459\n","  validation loss:\t\t0.013792\n","  top 1 accuracy:\t\t75.70 %\n","  top 5 accuracy:\t\t98.80 %\n","0.005421933252364397\n","Batch of classes 1 out of 10 batches\n","Epoch 37 of 70 took 6.292s\n","  training loss:\t\t0.006391\n","  validation loss:\t\t0.008787\n","  top 1 accuracy:\t\t83.00 %\n","  top 5 accuracy:\t\t99.20 %\n","0.005187625996768475\n","Batch of classes 1 out of 10 batches\n","Epoch 38 of 70 took 5.570s\n","  training loss:\t\t0.006095\n","  validation loss:\t\t0.009259\n","  top 1 accuracy:\t\t83.30 %\n","  top 5 accuracy:\t\t99.00 %\n","0.0058805812150239944\n","Batch of classes 1 out of 10 batches\n","Epoch 39 of 70 took 5.414s\n","  training loss:\t\t0.005993\n","  validation loss:\t\t0.009381\n","  top 1 accuracy:\t\t83.60 %\n","  top 5 accuracy:\t\t99.10 %\n","0.007972681894898415\n","Batch of classes 1 out of 10 batches\n","Epoch 40 of 70 took 6.615s\n","  training loss:\t\t0.007258\n","  validation loss:\t\t0.009332\n","  top 1 accuracy:\t\t82.70 %\n","  top 5 accuracy:\t\t99.10 %\n","0.0049576302990317345\n","Batch of classes 1 out of 10 batches\n","Epoch 41 of 70 took 5.286s\n","  training loss:\t\t0.005674\n","  validation loss:\t\t0.009708\n","  top 1 accuracy:\t\t82.80 %\n","  top 5 accuracy:\t\t98.60 %\n","0.004762091673910618\n","Batch of classes 1 out of 10 batches\n","Epoch 42 of 70 took 6.735s\n","  training loss:\t\t0.004812\n","  validation loss:\t\t0.010639\n","  top 1 accuracy:\t\t80.60 %\n","  top 5 accuracy:\t\t98.50 %\n","0.006720468867570162\n","Batch of classes 1 out of 10 batches\n","Epoch 43 of 70 took 5.407s\n","  training loss:\t\t0.004983\n","  validation loss:\t\t0.008532\n","  top 1 accuracy:\t\t83.50 %\n","  top 5 accuracy:\t\t99.00 %\n","0.004500655923038721\n","Batch of classes 1 out of 10 batches\n","Epoch 44 of 70 took 6.131s\n","  training loss:\t\t0.005215\n","  validation loss:\t\t0.014548\n","  top 1 accuracy:\t\t78.40 %\n","  top 5 accuracy:\t\t97.50 %\n","0.007176331244409084\n","Batch of classes 1 out of 10 batches\n","Epoch 45 of 70 took 5.915s\n","  training loss:\t\t0.006272\n","  validation loss:\t\t0.009979\n","  top 1 accuracy:\t\t83.30 %\n","  top 5 accuracy:\t\t98.80 %\n","0.0055407313629984856\n","Batch of classes 1 out of 10 batches\n","Epoch 46 of 70 took 5.264s\n","  training loss:\t\t0.005687\n","  validation loss:\t\t0.010808\n","  top 1 accuracy:\t\t82.40 %\n","  top 5 accuracy:\t\t98.70 %\n","0.004243508912622929\n","Batch of classes 1 out of 10 batches\n","Epoch 47 of 70 took 6.844s\n","  training loss:\t\t0.005482\n","  validation loss:\t\t0.009336\n","  top 1 accuracy:\t\t84.10 %\n","  top 5 accuracy:\t\t99.10 %\n","0.004275638144463301\n","Batch of classes 1 out of 10 batches\n","Epoch 48 of 70 took 5.402s\n","  training loss:\t\t0.004427\n","  validation loss:\t\t0.012442\n","  top 1 accuracy:\t\t80.10 %\n","  top 5 accuracy:\t\t98.40 %\n","0.004357766360044479\n","Batch of classes 1 out of 10 batches\n","Epoch 49 of 70 took 6.413s\n","  training loss:\t\t0.004726\n","  validation loss:\t\t0.009822\n","  top 1 accuracy:\t\t83.80 %\n","  top 5 accuracy:\t\t98.90 %\n","0.0030997167341411114\n","Batch of classes 1 out of 10 batches\n","Epoch 50 of 70 took 5.383s\n","  training loss:\t\t0.003489\n","  validation loss:\t\t0.007059\n","  top 1 accuracy:\t\t87.90 %\n","  top 5 accuracy:\t\t99.30 %\n","0.0030694245360791683\n","Batch of classes 1 out of 10 batches\n","Epoch 51 of 70 took 5.454s\n","  training loss:\t\t0.002876\n","  validation loss:\t\t0.007016\n","  top 1 accuracy:\t\t87.20 %\n","  top 5 accuracy:\t\t99.40 %\n","0.0025452689733356237\n","Batch of classes 1 out of 10 batches\n","Epoch 52 of 70 took 6.841s\n","  training loss:\t\t0.002718\n","  validation loss:\t\t0.007125\n","  top 1 accuracy:\t\t86.50 %\n","  top 5 accuracy:\t\t99.30 %\n","0.002982874633744359\n","Batch of classes 1 out of 10 batches\n","Epoch 53 of 70 took 5.320s\n","  training loss:\t\t0.002737\n","  validation loss:\t\t0.007069\n","  top 1 accuracy:\t\t88.60 %\n","  top 5 accuracy:\t\t99.20 %\n","0.0027047425974160433\n","Batch of classes 1 out of 10 batches\n","Epoch 54 of 70 took 6.815s\n","  training loss:\t\t0.002553\n","  validation loss:\t\t0.006842\n","  top 1 accuracy:\t\t88.30 %\n","  top 5 accuracy:\t\t99.40 %\n","0.0026445670519024134\n","Batch of classes 1 out of 10 batches\n","Epoch 55 of 70 took 5.260s\n","  training loss:\t\t0.002622\n","  validation loss:\t\t0.007082\n","  top 1 accuracy:\t\t88.10 %\n","  top 5 accuracy:\t\t99.50 %\n","0.0026206618640571833\n","Batch of classes 1 out of 10 batches\n","Epoch 56 of 70 took 5.980s\n","  training loss:\t\t0.002737\n","  validation loss:\t\t0.007607\n","  top 1 accuracy:\t\t86.80 %\n","  top 5 accuracy:\t\t99.40 %\n","0.002672292524948716\n","Batch of classes 1 out of 10 batches\n","Epoch 57 of 70 took 6.153s\n","  training loss:\t\t0.003017\n","  validation loss:\t\t0.007458\n","  top 1 accuracy:\t\t88.00 %\n","  top 5 accuracy:\t\t99.10 %\n","0.0023627120535820723\n","Batch of classes 1 out of 10 batches\n","Epoch 58 of 70 took 5.636s\n","  training loss:\t\t0.002614\n","  validation loss:\t\t0.007599\n","  top 1 accuracy:\t\t87.80 %\n","  top 5 accuracy:\t\t99.30 %\n","0.002412488916888833\n","Batch of classes 1 out of 10 batches\n","Epoch 59 of 70 took 8.464s\n","  training loss:\t\t0.002730\n","  validation loss:\t\t0.007373\n","  top 1 accuracy:\t\t87.10 %\n","  top 5 accuracy:\t\t99.30 %\n","0.0018574411515146494\n","Batch of classes 1 out of 10 batches\n","Epoch 60 of 70 took 5.326s\n","  training loss:\t\t0.002157\n","  validation loss:\t\t0.007099\n","  top 1 accuracy:\t\t88.90 %\n","  top 5 accuracy:\t\t99.30 %\n","0.0015301884850487113\n","Batch of classes 1 out of 10 batches\n","Epoch 61 of 70 took 6.823s\n","  training loss:\t\t0.001988\n","  validation loss:\t\t0.007501\n","  top 1 accuracy:\t\t87.70 %\n","  top 5 accuracy:\t\t99.20 %\n","0.0018700080690905452\n","Batch of classes 1 out of 10 batches\n","Epoch 62 of 70 took 5.424s\n","  training loss:\t\t0.001734\n","  validation loss:\t\t0.007367\n","  top 1 accuracy:\t\t88.30 %\n","  top 5 accuracy:\t\t99.30 %\n","0.0020230093505233526\n","Batch of classes 1 out of 10 batches\n","Epoch 63 of 70 took 6.466s\n","  training loss:\t\t0.001680\n","  validation loss:\t\t0.007410\n","  top 1 accuracy:\t\t88.80 %\n","  top 5 accuracy:\t\t99.00 %\n","0.0012067630887031555\n","Batch of classes 1 out of 10 batches\n","Epoch 64 of 70 took 5.728s\n","  training loss:\t\t0.001643\n","  validation loss:\t\t0.007338\n","  top 1 accuracy:\t\t88.70 %\n","  top 5 accuracy:\t\t99.20 %\n","0.0014157318510115147\n","Batch of classes 1 out of 10 batches\n","Epoch 65 of 70 took 5.416s\n","  training loss:\t\t0.001622\n","  validation loss:\t\t0.007430\n","  top 1 accuracy:\t\t88.80 %\n","  top 5 accuracy:\t\t99.20 %\n","0.0008877282962203026\n","Batch of classes 1 out of 10 batches\n","Epoch 66 of 70 took 6.999s\n","  training loss:\t\t0.001389\n","  validation loss:\t\t0.007512\n","  top 1 accuracy:\t\t88.70 %\n","  top 5 accuracy:\t\t99.10 %\n","0.0013933065347373486\n","Batch of classes 1 out of 10 batches\n","Epoch 67 of 70 took 5.418s\n","  training loss:\t\t0.001798\n","  validation loss:\t\t0.007527\n","  top 1 accuracy:\t\t87.50 %\n","  top 5 accuracy:\t\t99.20 %\n","0.0009757748921401799\n","Batch of classes 1 out of 10 batches\n","Epoch 68 of 70 took 6.984s\n","  training loss:\t\t0.001467\n","  validation loss:\t\t0.007511\n","  top 1 accuracy:\t\t87.70 %\n","  top 5 accuracy:\t\t99.20 %\n","0.0012806318700313568\n","Batch of classes 1 out of 10 batches\n","Epoch 69 of 70 took 5.372s\n","  training loss:\t\t0.001415\n","  validation loss:\t\t0.007404\n","  top 1 accuracy:\t\t88.60 %\n","  top 5 accuracy:\t\t99.20 %\n","0.0018096909625455737\n","Batch of classes 1 out of 10 batches\n","Epoch 70 of 70 took 6.292s\n","  training loss:\t\t0.001749\n","  validation loss:\t\t0.007319\n","  top 1 accuracy:\t\t88.20 %\n","  top 5 accuracy:\t\t99.20 %\n","Updating exemplar set...\n","Computing mean-of_exemplars and theoretical mean...\n","Computing accuracy on the original batch of classes...\n","Final results on original classes:\n","  top 1 accuracy iCaRL          :\t\t88.50 %\n","  top 1 accuracy Hybrid 1       :\t\t88.20 %\n","  top 1 accuracy NCM            :\t\t88.40 %\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"]},{"output_type":"stream","name":"stdout","text":["Final results on cumul of classes:\n","  top 1 accuracy iCaRL          :\t\t88.50 %\n","  top 1 accuracy Hybrid 1       :\t\t88.20 %\n","  top 1 accuracy NCM            :\t\t88.40 %\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Classes in this batch: tensor([94, 92, 10, 72, 49, 78, 61, 14,  8, 86])\n","\n","\n","Before first epoch\n","  validation loss:\t\t0.143128\n","  top 1 accuracy:\t\t0.00 %\n","  top 5 accuracy:\t\t0.00 %\n","Batch of classes number 2 arrives ...\n","0.10054506361484528\n","Batch of classes 2 out of 10 batches\n","Epoch 1 of 70 took 8.287s\n","  training loss:\t\t0.042236\n","  validation loss:\t\t0.048693\n","  top 1 accuracy:\t\t11.30 %\n","  top 5 accuracy:\t\t76.40 %\n","0.028051959350705147\n","Batch of classes 2 out of 10 batches\n","Epoch 2 of 70 took 6.863s\n","  training loss:\t\t0.027256\n","  validation loss:\t\t0.040286\n","  top 1 accuracy:\t\t22.20 %\n","  top 5 accuracy:\t\t82.80 %\n","0.027767594903707504\n","Batch of classes 2 out of 10 batches\n","Epoch 3 of 70 took 8.486s\n","  training loss:\t\t0.025438\n","  validation loss:\t\t0.039016\n","  top 1 accuracy:\t\t28.00 %\n","  top 5 accuracy:\t\t88.30 %\n","0.023165153339505196\n","Batch of classes 2 out of 10 batches\n","Epoch 4 of 70 took 6.874s\n","  training loss:\t\t0.024199\n","  validation loss:\t\t0.037017\n","  top 1 accuracy:\t\t36.60 %\n","  top 5 accuracy:\t\t90.00 %\n","0.02541900984942913\n","Batch of classes 2 out of 10 batches\n","Epoch 5 of 70 took 8.664s\n","  training loss:\t\t0.023691\n","  validation loss:\t\t0.033834\n","  top 1 accuracy:\t\t39.00 %\n","  top 5 accuracy:\t\t89.80 %\n","0.019251342862844467\n","Batch of classes 2 out of 10 batches\n","Epoch 6 of 70 took 6.915s\n","  training loss:\t\t0.022795\n","  validation loss:\t\t0.037149\n","  top 1 accuracy:\t\t36.60 %\n","  top 5 accuracy:\t\t89.80 %\n","0.01907438412308693\n","Batch of classes 2 out of 10 batches\n","Epoch 7 of 70 took 8.289s\n","  training loss:\t\t0.022502\n","  validation loss:\t\t0.036738\n","  top 1 accuracy:\t\t36.30 %\n","  top 5 accuracy:\t\t88.40 %\n","0.018854880705475807\n","Batch of classes 2 out of 10 batches\n","Epoch 8 of 70 took 7.833s\n","  training loss:\t\t0.021559\n","  validation loss:\t\t0.036391\n","  top 1 accuracy:\t\t43.70 %\n","  top 5 accuracy:\t\t89.90 %\n","0.02376367338001728\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-566d10bf7bd6>\u001b[0m in \u001b[0;36m<cell line: 392>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m     \u001b[0;31m# top1_acc_list을 이용하여 그래프 그리기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mplot_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop1_acc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-566d10bf7bd6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Lines 171-186: And a full pass over the validation data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# 현재 모델을 사용하여 검증 데이터셋에서 정확도를 계산합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             acc_result, val_err, _, _ = get_accuracy(model, task_info.get_current_test_set(),  device=device,\n\u001b[0m\u001b[1;32m    225\u001b[0m                                                      \u001b[0mrequired_top_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_detailed_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                                                      \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_one_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/iCaRL/icarl-pytorch-master/cl_metrics_tools.py\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(model, test_dataset, device, required_top_k, return_detailed_outputs, criterion, make_one_hot, n_classes, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/iCaRL/icarl-pytorch-master/models/icarl_net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;31m# 모델의 forward 연산을 정의하는 함수이며, 입력 x를 받아 출력을 계산한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Already flattened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;31m# 입력 x를 feature_extractor에 통과시켜 특성을 추출합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# 이미지 차원이 1x1 이 되기 때문에 Flatten이 적용되어 이미지를 벡터로 변환한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/iCaRL/icarl-pytorch-master/models/icarl_net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2432\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm3d\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2433\u001b[0m     \"\"\"\n\u001b[0;32m-> 2434\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2435\u001b[0m         return handle_torch_function(\n\u001b[1;32m   2436\u001b[0m             \u001b[0mbatch_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","from functools import partial\n","from typing import Callable, Tuple, List\n","\n","import numpy as np\n","import torch\n","from math import ceil\n","from torch import Tensor\n","from torch.nn import BCELoss\n","from torch.optim.lr_scheduler import MultiStepLR\n","from torchvision.datasets.cifar import CIFAR100\n","from cl_dataset_tools import NCProtocol, NCProtocolIterator, TransformationDataset\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.utils.data import Dataset, ConcatDataset\n","import torchvision.transforms as transforms\n","\n","from cl_strategies import icarl_accuracy_measure, icarl_cifar100_augment_data\n","from models import make_icarl_net\n","from cl_metrics_tools import get_accuracy\n","from models.icarl_net import IcarlNet, initialize_icarl_net\n","from utils import get_dataset_per_pixel_mean, make_theano_training_function, make_theano_validation_function, \\\n","    make_theano_feature_extraction_function, make_theano_inference_function, make_batch_one_hot\n","\n","\n","def main():\n","    # This script tries to reprodice results of official iCaRL code\n","    # https://github.com/srebuffi/iCaRL/blob/master/iCaRL-TheanoLasagne/main_cifar_100_theano.py\n","\n","    ######### Modifiable Settings ##########\n","    batch_size = 128            # Batch size\n","    n          = 5              # Set the depth of the architecture: n = 5 -> 32 layers (See He et al. paper)\n","    # nb_val     = 0            # Validation samples per class\n","    nb_cl      = 10             # Classes per group (한 그룹 당 클래스 수)\n","    nb_protos  = 20             # Number of prototypes per class at the end: total protoset memory/ total number of classes\n","    epochs     = 70             # Total number of epochs\n","    lr_old     = 2.             # Initial learning rate\n","    lr_strat   = [49, 63]       # Epochs where learning rate gets decreased\n","    lr_factor  = 5.             # Learning rate decrease factor ( 학습률 감소 배율)\n","    wght_decay = 0.00001        # Weight Decay (가중치 감소)\n","    nb_runs    = 1              # Number of runs (random ordering of classes at each run)\n","    torch.manual_seed(1993)     # Fix the random seed\n","    ########################################\n","\n","    fixed_class_order = [87,  0, 52, 58, 44, 91, 68, 97, 51, 15,\n","                         94, 92, 10, 72, 49, 78, 61, 14,  8, 86,\n","                         84, 96, 18, 24, 32, 45, 88, 11,  4, 67,\n","                         69, 66, 77, 47, 79, 93, 29, 50, 57, 83,\n","                         17, 81, 41, 12, 37, 59, 25, 20, 80, 73,\n","                          1, 28,  6, 46, 62, 82, 53,  9, 31, 75,\n","                         38, 63, 33, 74, 27, 22, 36,  3, 16, 21,\n","                         60, 19, 70, 90, 89, 43,  5, 42, 65, 76,\n","                         40, 30, 23, 85,  2, 95, 56, 48, 71, 64,\n","                         98, 13, 99,  7, 34, 55, 54, 26, 35, 39]\n","\n","    # fixed_class_order = None\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Line 31: Load the dataset\n","    # Asserting nb_val == 0 equals to full cifar100\n","    # That is, we only declare transformations here\n","    # Notes: dstack and reshape already done inside CIFAR100 class\n","    # Mean is calculated on already scaled (by /255) images\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),  # ToTensor scales from [0, 255] to [0, 1.0]\n","    ])\n","\n","    per_pixel_mean = get_dataset_per_pixel_mean(CIFAR100('./data/cifar100', train=True, download=True,\n","                                                         transform=transform))\n","\n","    # https://github.com/srebuffi/iCaRL/blob/90ac1be39c9e055d9dd2fa1b679c0cfb8cf7335a/iCaRL-TheanoLasagne/utils_cifar100.py#L146\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        lambda img_pattern: img_pattern - per_pixel_mean, #(이미지를 텐서로 변환 후, 픽셀별 평균을 뺀다),\n","        icarl_cifar100_augment_data#(데이터 증강을 수행)),\n","    ])\n","\n","    # Must invert previous ToTensor(), otherwise RandomCrop and RandomHorizontalFlip won't work\n","    transform_prototypes = transforms.Compose([\n","        icarl_cifar100_augment_data,\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        lambda img_pattern: img_pattern - per_pixel_mean,  # Subtract per-pixel mean\n","    ])\n","\n","    # Line 43: Initialization\n","    dictionary_size = 500\n","    top1_acc_list_cumul = torch.zeros(100//nb_cl, 3, nb_runs)\n","    top1_acc_list_ori = torch.zeros(100//nb_cl, 3, nb_runs)\n","\n","    # Line 48: # Launch the different runs\n","    # Skipped as this script will only manage singe runs\n","\n","    # Lines 51, 52, 54 already managed in NCProtocol\n","\n","    protocol = NCProtocol(CIFAR100('./data/cifar100', train=True, download=True, transform=transform),\n","                          CIFAR100('./data/cifar100', train=False, download=True, transform=transform_test),\n","                          n_tasks=100//nb_cl, shuffle=True, seed=None, fixed_class_order=fixed_class_order)\n","\n","    model: IcarlNet = make_icarl_net(num_classes=100)\n","    model.apply(initialize_icarl_net)\n","\n","    model = model.to(device)\n","\n","    criterion = BCELoss()  # Line 66-67 이진교차엔트로피 함수 생성\n","\n","    # Line 74, 75\n","    # Note: sh_lr is a theano \"shared\"\n","    sh_lr = lr_old\n","\n","    # noinspection PyTypeChecker, 검증 함수 정의\n","    val_fn: Callable[[Tensor, Tensor],\n","                     Tuple[Tensor, Tensor, Tensor]] = partial(make_theano_validation_function, model,\n","                                                              BCELoss(), 'feature_extractor',\n","                                                              device=device)\n","\n","    # noinspection PyTypeChecker\n","    function_map: Callable[[Tensor], Tensor] = partial(make_theano_feature_extraction_function, model,\n","                                                       'feature_extractor', device=device, batch_size=batch_size)\n","\n","    # Lines 90-97: Initialization of the variables for this run\n","\n","    x_protoset_cumuls: List[Tensor] = []\n","    y_protoset_cumuls: List[Tensor] = []\n","    alpha_dr_herding = torch.zeros((100 // nb_cl, dictionary_size, nb_cl), dtype=torch.float)\n","\n","    # Lines 101-103: already managed by NCProtocol/NCProtocolIterator\n","\n","    train_dataset: Dataset\n","    task_info: NCProtocolIterator\n","\n","    func_pred: Callable[[Tensor], Tensor] #예측하는 변수 초기화\n","    # func_pred_feat: Callable[[Tensor], Tensor] # Unused\n","\n","    for task_idx, (train_dataset, task_info) in enumerate(protocol): # protocol을 순회하면서 반복문을 실행하면서 동시에 각 반복 요소에 train, test idx를 저장하는 것\n","        print('Classes in this batch:', task_info.classes_in_this_task) # 현재 테스트에 포함된 클래스를 출력\n","\n","        # Lines 107, 108: Save data results at each increment\n","        # accuracy 리스트를 파일에 저장\n","        torch.save(top1_acc_list_cumul, 'top1_acc_list_cumul_icarl_cl' + str(nb_cl))\n","        torch.save(top1_acc_list_ori, 'top1_acc_list_ori_icarl_cl' + str(nb_cl))\n","\n","        # Note: lines 111-125 already managed in NCProtocol/NCProtocolIterator\n","\n","        # Lines 128-135: Add the stored exemplars to the training data\n","        # Note: X_valid_ori and Y_valid_ori already managed in NCProtocol/NCProtocolIterator\n","        if task_idx != 0:\n","          # 첫 번째 task가 아닌 경우, 이전 task의 프로토셋을 현재 학습 데이터셋에 추가\n","            protoset = TransformationDataset(TensorDataset(torch.cat(x_protoset_cumuls), torch.cat(y_protoset_cumuls)),\n","                                             transform=transform_prototypes, target_transform=None)\n","            train_dataset = ConcatDataset((train_dataset, protoset))\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n","\n","        # Line 137: # Launch the training loop\n","        # From lines: 69, 70, 76, 77\n","        # Note optimizer == train_fn\n","        # weight_decay == l2_penalty\n","        optimizer = torch.optim.SGD(model.parameters(), lr=sh_lr, weight_decay=wght_decay, momentum=0.9)\n","        train_fn = partial(make_theano_training_function, model, criterion, optimizer, device=device)\n","        # 함수를 부분적으로 적용하여 학습 함수 train_fc을 생성합니다. 이 함수는 주어진 모델, 손실 함수, 최적화기, 디바이스를 사용하여 학습을 수행합니다.\n","        scheduler = MultiStepLR(optimizer, lr_strat, gamma=1.0/lr_factor)\n","        # 다단계 학습률 스케쥴러인 MultiStepLR을 생성합니다. 이 스케쥴러는 주어진 에폭에서 학습률을 감소시키는데 사용합니다. lr_strat 에서 지정한 에폭에서 학습률을 gamma 값으로 나누어 줄입니다.\n","\n","        print(\"\\n\")\n","\n","        # Added (not found in original code): validation accuracy before first epoch\n","        acc_result, val_err, _, _ = get_accuracy(model, task_info.get_current_test_set(), device=device,\n","                                                 required_top_k=[1, 5], return_detailed_outputs=False,\n","                                                 criterion=BCELoss(), make_one_hot=True, n_classes=100,\n","                                                 batch_size=batch_size, shuffle=False, num_workers=8)\n","        print(\"Before first epoch\")\n","        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err))  # Note: already averaged\n","        print(\"  top 1 accuracy:\\t\\t{:.2f} %\".format(acc_result[0].item() * 100))\n","        print(\"  top 5 accuracy:\\t\\t{:.2f} %\".format(acc_result[1].item() * 100))\n","        # End of added code\n","\n","        print('Batch of classes number {0} arrives ...'.format(task_idx + 1))\n","\n","        # Sets model in train mode\n","        model.train() # Start train\n","        for epoch in range(epochs):\n","            # Note: already shuffled (line 143-146)\n","\n","            # Lines 148-150\n","            train_err: float = 0\n","            train_batches: int = 0\n","            start_time: float = time.time()\n","            # 학습 손실과 미니 배치수 초기화, 학습 시작 시간을 기록\n","\n","            patterns: Tensor\n","            labels: Tensor\n","            for patterns, labels in train_loader:  # Line 151\n","                # Lines 153-154\n","                targets = make_batch_one_hot(labels, 100)\n","                # 레이블을 one-hot 인코딩하여 targets로 변환. 클래수의 수는 100\n","\n","                old_train = train_err  # Line 155\n","\n","                targets = targets.to(device)\n","                patterns = patterns.to(device)\n","\n","                if task_idx == 0:   # Line 156\n","                # 첫 번째 task인 경우 train_fn 함수를 사용하여 학습을 진행\n","                    train_err += train_fn(patterns, targets)  # Line 157\n","\n","\n","                # Lines 160-163: Distillation\n","                # 두 번째 이후 task인 경우에는 이전 task의 모델을 사용하여 예측을 수행하고, 이전 클래스에 대한 예측 결과를 targets에 업데이트 한 후, 학습을 진행합니다.\n","                if task_idx > 0:\n","                    prediction_old = func_pred(patterns)\n","                    targets[:, task_info.prev_classes] = prediction_old[:, task_info.prev_classes]\n","                    train_err += train_fn(patterns, targets)\n","                # 매 100번째 미니배치마다 학습 손실의 변화를 출력합니다.\n","                if (train_batches % 100) == 1:\n","                    print(train_err - old_train)\n","                # 미니배치 수를 증가시킨다.\n","                train_batches += 1\n","\n","            # Lines 171-186: And a full pass over the validation data:\n","            # 현재 모델을 사용하여 검증 데이터셋에서 정확도를 계산합니다.\n","            acc_result, val_err, _, _ = get_accuracy(model, task_info.get_current_test_set(),  device=device,\n","                                                     required_top_k=[1, 5], return_detailed_outputs=False,\n","                                                     criterion=BCELoss(), make_one_hot=True, n_classes=100,\n","                                                     batch_size=batch_size, shuffle=False, num_workers=8)\n","\n","            # Lines 188-202: Then we print the results for this epoch:\n","            # 현재 에폭에서의 학습 손실, 검증 손실, 정확도를 출력합니다.\n","            print(\"Batch of classes {} out of {} batches\".format(\n","                task_idx + 1, 100 // nb_cl))\n","            print(\"Epoch {} of {} took {:.3f}s\".format(\n","                epoch + 1,\n","                epochs,\n","                time.time() - start_time))\n","            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n","            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err))  # Note: already averaged\n","            print(\"  top 1 accuracy:\\t\\t{:.2f} %\".format(\n","                acc_result[0].item() * 100))\n","            print(\"  top 5 accuracy:\\t\\t{:.2f} %\".format(\n","                acc_result[1].item() * 100))\n","            # adjust learning rate, 스케쥴러를 호출하여 학습률을 조정한다.\n","            scheduler.step()\n","\n","        # Lines 205-213: Duplicate current network to distillate info\n","        # 첫 번째 task인 경우 새로운 model2를 생성합니다. make_icarl_net 함수를 사용하여 모델을 생성하고, 디바이스에 할당합니다.\n","        # func_pred 함수는 model2를 사용하여 예측을 수행\n","        if task_idx == 0:\n","            model2 = make_icarl_net(100, n=n)\n","            model2 = model2.to(device)\n","            # noinspection PyTypeChecker\n","            func_pred = partial(make_theano_inference_function, model2, device=device)\n","\n","            # Note: func_pred_feat is unused\n","            # func_pred_feat = partial(make_theano_feature_extraction_function, model=model2,\n","            #                          feature_extraction_layer='feature_extractor')\n","\n","        model2.load_state_dict(model.state_dict())\n","        # model2에 model1의 상태 사전을 로드한다.\n","\n","        # Lines 216, 217: Save the network\n","        torch.save(model.state_dict(), 'net_incr'+str(task_idx+1)+'_of_'+str(100//nb_cl))\n","        torch.save(model.feature_extractor.state_dict(), 'intermed_incr'+str(task_idx+1)+'_of_'+str(100//nb_cl))\n","        # 모델과 특성 추출기의 상태 사전을 파일로 저장\n","\n","        # Lines 220-242: Exemplars\n","        nb_protos_cl = int(ceil(nb_protos * 100. / nb_cl / (task_idx + 1)))\n","        # 각 클래스에 대한 프로토셋 크기를 계산\n","\n","\n","        # Herding\n","        print('Updating exemplar set...')\n","        for iter_dico in range(nb_cl):\n","            # Possible exemplars in the feature space and projected on the L2 sphere\n","            prototypes_for_this_class, _ = task_info.swap_transformations() \\\n","                .get_current_training_set()[iter_dico*dictionary_size:(iter_dico+1)*dictionary_size]\n","\n","            mapped_prototypes: Tensor = function_map(prototypes_for_this_class)\n","            D: Tensor = mapped_prototypes.T\n","            D = D / torch.norm(D, dim=0)\n","\n","            # Herding procedure : ranking of the potential exemplars\n","            mu = torch.mean(D, dim=1)\n","            alpha_dr_herding[task_idx, :, iter_dico] = alpha_dr_herding[task_idx, :, iter_dico] * 0\n","            w_t = mu\n","            iter_herding = 0\n","            iter_herding_eff = 0\n","            while not (torch.sum(alpha_dr_herding[task_idx, :, iter_dico] != 0) ==\n","                       min(nb_protos_cl, 500)) and iter_herding_eff < 1000:\n","                tmp_t = torch.mm(w_t.unsqueeze(0), D)\n","                ind_max = torch.argmax(tmp_t)\n","                iter_herding_eff += 1\n","                if alpha_dr_herding[task_idx, ind_max, iter_dico] == 0:\n","                    alpha_dr_herding[task_idx, ind_max, iter_dico] = 1 + iter_herding\n","                    iter_herding += 1\n","                w_t = w_t + mu - D[:, ind_max]\n","          # 각 클래스에 대한 프로토타입을 업데이트 한다. 프로토 타입은 특성 공간에서 가능한 후보군으로 생성된다. 프로토 타입은 평균 벡터인 mu 를 기준으로 한 Herding 알고리즘에 따라\n","          # 랭킹을 매기며 선택된다.\n","        # Lines 244-246: Prepare the protoset\n","        # 누적 프로토셋과 관련된 변수를 초기화한다.\n","        x_protoset_cumuls: List[Tensor] = []\n","        y_protoset_cumuls: List[Tensor] = []\n","\n","        # Lines 249-276: Class means for iCaRL and NCM + Storing the selected exemplars in the protoset\n","        print('Computing mean-of_exemplars and theoretical mean...')\n","        class_means = torch.zeros((64, 100, 2), dtype=torch.float)\n","        for iteration2 in range(task_idx + 1):\n","            for iter_dico in range(nb_cl):\n","                prototypes_for_this_class: Tensor\n","                current_cl = task_info.classes_seen_so_far[list(\n","                    range(iteration2 * nb_cl, (iteration2 + 1) * nb_cl))]\n","                current_class = current_cl[iter_dico].item()\n","\n","                prototypes_for_this_class, _ = task_info.swap_transformations().get_task_training_set(iteration2)[\n","                                            iter_dico * dictionary_size:(iter_dico + 1)*dictionary_size]\n","\n","                # Collect data in the feature space for each class\n","                mapped_prototypes: Tensor = function_map(prototypes_for_this_class)\n","                D: Tensor = mapped_prototypes.T\n","                D = D / torch.norm(D, dim=0)\n","\n","                # Flipped version also\n","                # PyTorch doesn't support ::-1 yet\n","                # And with \"yet\" I mean: PyTorch will NEVER support ::-1\n","                # See: https://github.com/pytorch/pytorch/issues/229 (<-- year 2016!)\n","                # Also: https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663\n","                mapped_prototypes2: Tensor = function_map(torch.from_numpy(\n","                    prototypes_for_this_class.numpy()[:, :, :, ::-1].copy()))\n","                D2: Tensor = mapped_prototypes2.T\n","                D2 = D2 / torch.norm(D2, dim=0)\n","\n","                # iCaRL\n","                alph = alpha_dr_herding[iteration2, :, iter_dico]\n","                alph = (alph > 0) * (alph < nb_protos_cl + 1) * 1.\n","\n","                # Adds selected replay patterns\n","                x_protoset_cumuls.append(prototypes_for_this_class[torch.where(alph == 1)[0]])\n","                # Appends labels of replay patterns -> Tensor([current_class, current_class, current_class, ...])\n","                y_protoset_cumuls.append(current_class * torch.ones(len(torch.where(alph == 1)[0])))\n","                alph = alph / torch.sum(alph)\n","                class_means[:, current_cl[iter_dico], 0] = (torch.mm(D, alph.unsqueeze(1)).squeeze(1) +\n","                                                            torch.mm(D2, alph.unsqueeze(1)).squeeze(1)) / 2\n","                class_means[:, current_cl[iter_dico], 0] /= torch.norm(class_means[:, current_cl[iter_dico], 0])\n","\n","                # Normal NCM\n","                alph = torch.ones(dictionary_size) / dictionary_size\n","                class_means[:, current_cl[iter_dico], 1] = (torch.mm(D, alph.unsqueeze(1)).squeeze(1) +\n","                                                            torch.mm(D2, alph.unsqueeze(1)).squeeze(1)) / 2\n","\n","                class_means[:, current_cl[iter_dico], 1] /= torch.norm(class_means[:, current_cl[iter_dico], 1])\n","\n","        torch.save(class_means, 'cl_means')  # Line 278\n","\n","        # Calculate validation error of model on the first nb_cl classes:\n","        print('Computing accuracy on the original batch of classes...')\n","        top1_acc_list_ori = icarl_accuracy_measure(task_info.get_task_test_set(0), class_means, val_fn,\n","                                                   top1_acc_list_ori, task_idx, 0, 'original',\n","                                                   make_one_hot=True, n_classes=100,\n","                                                   batch_size=batch_size, num_workers=8)\n","\n","        top1_acc_list_cumul = icarl_accuracy_measure(task_info.get_cumulative_test_set(), class_means, val_fn,\n","                                                     top1_acc_list_cumul, task_idx, 0, 'cumul of',\n","                                                     make_one_hot=True, n_classes=100,\n","                                                     batch_size=batch_size, num_workers=8)\n","\n","\n","\n","\n","\n","    # Final save of the data\n","    torch.save(top1_acc_list_cumul, 'top1_acc_list_cumul_icarl_cl' + str(nb_cl))\n","    torch.save(top1_acc_list_ori, 'top1_acc_list_ori_icarl_cl' + str(nb_cl))\n","\n","\n","\n","if __name__ == '__main__':\n","    main()\n","    # top1_acc_list을 이용하여 그래프 그리기\n",""]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/iCaRL/icarl-pytorch-master\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHgzGPTCqUnw","executionInfo":{"status":"ok","timestamp":1688967286735,"user_tz":-540,"elapsed":395,"user":{"displayName":"안창준전자ㆍ제어공학과","userId":"03412762459924900416"}},"outputId":"c722cb4d-eada-406d-cb40-529a9c82df23"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/iCaRL/icarl-pytorch-master\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EIY-Ujond1_w"},"execution_count":null,"outputs":[]}]}